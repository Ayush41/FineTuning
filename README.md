# ğŸ§  finetune

**finetune** is an open-source research project focused on **fine-tuning Small Language Models (SLMs)** on **custom datasets** â€” locally and efficiently â€” using an **NVIDIA RTX 3040 GPU**.  
The goal of this project is to **benchmark and compare fine-tuning performance** across different SLM architectures while developing an open research paper on the findings.

---

## ğŸš€ Project Goals

- ğŸ”§ Fine-tune open-source language models on **custom, domain-specific data**.
- ğŸ’» Run all experiments **locally** (no cloud or API dependency).
- âš™ï¸ Optimize training for **consumer-grade GPUs** like the **RTX 3040**.
- ğŸ“Š Compare training efficiency, accuracy, and inference quality across various **SLMs**.
- ğŸ§¾ Publish an **open-access research paper** summarizing results, challenges, and insights.

---

## ğŸ§© Features

- Support for **popular open-source models** (e.g., LLaMA, Mistral, Phi, TinyLlama, etc.)
- Local **LoRA / QLoRA** fine-tuning support.
- Lightweight **training pipeline** with PyTorch and Hugging Face Transformers.
- Experiment tracking and evaluation metrics.
- Reproducible results with config-based setups.

---
